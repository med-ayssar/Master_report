\chapter{Discussion}
 \label{chap:disc}
 \section{Conclusions and Summary}

The aim of the thesis was to eliminate the explicit calls to the NESTML interface in the simulation script. The eliminated calls are then replaced by an automated workflow controlled by the JIT mechanism running in the background. Two major components are built in the JIT mechanism to make it work seamlessly without having a great impact on the simulation script. The main component is the Wrapper class that is responsible for intercepting the function calls in the NESTML interface and by either delaying their executions or applying certain pre- and post-processing steps before or after the real function calls.

The second component is the NodeCollectionProxy. Intercepting the call to the \texttt{Create} function, we were able to create a partial version of the NESTML model that provides functionalities to retrieve and update the state of the instances before really being instantiated from the complete NESTML model. The role of the NodeCollectionProxy is to make the retrieve and update calls for the partial version of the model work exactly as the complete models without the user noticing these changes. 

Due to the existence of the of partial instances of the NESTML model in the Python level and the real complete instances of the model in the C++ level, we double the memory requirements of the simulation script. To abate the memory requirements, we adjust the internal model representation in the NestKernel by making the model independent of the storage of its properties. The adjusted new representation supports vectorization by grouping all instances of the same model into one central unit also called a vectorized model. The attributes of the vectorized model are in a vector form, and their size is equal to the number of instances created during the simulation of that particular
model type. Additionally, the vectorization opens up possibilities for future optimizations, such as decomposing the generated C++ code of the NESTML block into independent blocks and incrementally assembling the models during the evolution of the simulation.

In this chapter, we give an overview of the main concepts introduced in the previous chapters and discuss potential improvements that could benefit from bringing the JIT approach and vectorization closer to each other.

\subsection*{JIT Module: The Wrapper}

In \autoref{chap:jit} we have designed and implemented a wrapper around PyNEST, which can intercept function calls, and depending on specific constraints, can either apply some pre-processing and post-processing steps before calling the original function, or disable the original function completely. The wrapper has a straightforward structure that renders its use simple and flexible. Each targeted function in PyNEST has a custom wrapper, and these wrappers are registered in a dictionary, with the keys being the name of the targeted function and the values of the registered classes. Once the JIT module is imported, it iterates over PyNEST's high-level functions, methods, and classes, and if one of these objects is registered in the dictionary, a wrapper instance is created and inserted into the JIT module. Non-targeted objects are inserted directly without a wrapper in between. Another main feature of the wrappers is that they are implemented independently and do not influence each other.

\subsection*{JIT Module: The lightweight version}
Since the implemented wrapper for the \texttt{Create()} function must search for the model, and in the worst case, the code of the models must be generated, we decided to split the code generation overhead. Upon a call to \texttt{Create()}, the user will not need to wait for the whole code of the model to be generated, built, and installed, but instead we create a \emph{lightweight} version of the model that only has the \emph{state} and \emph{parameter} blocks, but allows the user to parametrize it using deterministic or randomized values already before the full code is available. The real models will become available once the user calls the \texttt{Connect()} function.

One drawback with this approach is that after calling \texttt{Connect()} or \texttt{Simulate()} when the full models are available, the model attributes have to be copied from the lightweight version to the real instances, which leads to a duplication of memory, once on the Python level and once on the C++ level.

\subsection*{JIT Module: The NodeCollectionProxy}

In order to retrieve information from the lightweight version and make the handling of the code generation transparent and similar to PyNEST's \texttt{NodeCollection}, we have implemented the \texttt{JitNodeCollection} and the \texttt{NodeCollectionProxy}. The relation between these three elements is a tree structure, in which the \texttt{NodeCollectionProxy} is the root element. It has two children, the first being the \texttt{JitNodeCollection} and the second being the \texttt{NodeCollection}. The \texttt{JitNodeCollection} has an arbitrary number of children. If the IDs of the children of the \texttt{JitNodeCollection} build a contiguous space, and they belong to the same model, they are represented by the \texttt{JitNode}, which represents the leaves of the tree. The \texttt{NodeCollectionProxy}, the \texttt{JitNodeCollection} and the \texttt{JitNode} share the same interface that supports retrieving and setting the attributes of the instances as well as \emph{indexing} and \emph{slicing} exactly like the provided functionalities in the \texttt{NodeCollection}.

\subsection*{Vectorization}

To address the problem with memory duplication explained above, we have introduced a new data structure in the NestKernel for handling models. The new data structure is based on the idea of a \emph{structure of arrays (SoA)}, where instead of having $n$ instances of the same model with $k$ attributes, we have exactly one instance that has $k$ attributes in the form of \emph{vectors} and each of these vectors has a size of $n$. The vectorization of models behaves like a container of nodes, so instead of updating the nodes separately, we can now tell the containers to update the states of all nodes in parallel by utilizing the efficiency of \emph{SIMD} instructions in modern processors. The vectorization should also decrease the amount of \emph{cache misses}, as updating a container exploits the spatial locality of the items in the array and thus requires fewer data transfers from main memory.

\subsection*{Performance}

The performance of the JIT module without using an external synapse model does not deviate a lot from running the simulation script without the JIT tool. However, when using an external custom synapse, we observe a significant increase in runtime. The difference can be explained by the overhead of running some steps of the code generation pipeline twice, maintaining the functionality of the \texttt{NodeCollectionProxy} and finally replacing the network nodes and updating their parameters. The fact that the vectorization support in NestKernel is currently not leveraging the full potential is reflected by the fact that the performance of the \emph{vectorized} models and the \emph{non-vectorized} models are still quite similar on average.

\section{Outlook}

\subsection*{Modularity}

As mentioned multiple times already, the duplication of memory between the Python and C++ sides because of the lightweight version of the model is a severe problem that is only partially solved by the current vectorization strategy. Moreover, the current version of the \texttt{NodeCollectionProxy} requires that each change in PyNEST's \texttt{NodeCollection} must be followed, which leads to an increased maintainance burden for the JIT project. To mitigate these problems, we can improve the concept of the lightweight version and make it more flexible by adjusting the code generation pipeline and making it generate and build the library in pieces. The model could, for example, be split into two pieces: The first would contain the state and parameters blocks, which could be generated and built upon the call to \texttt{Create()}. The remaining code could then be generated in the background and connected with the first part when calling the \texttt{Connect()} or \texttt{Simulate()} functions. Since the co-generation of the neuron and synapse just moves variables from the synapse to the neuron, we may then generate the code of the synapse and also split it into two parts: the first part would include the moved variables in a separate library and append them to the neuron state block during the simulation. The second part would take care of the rest of the functionality in the synapse and make it available during the \texttt{Connect()} call.

\subsection*{Removal of the NodeCollectionProxy}

With the extended \emph{modularity} as outlined above in place, we could completely drop the use of the current data structures required for the JIT mechanism (i.e., the \texttt{NodeCollectionProxy}, \texttt{JitNodeCollection} and \texttt{JitNode}). The NestKernel infrastructure would then be adjusted to support the modularity of the models and make it possible to assemble the pieces of the model at runtime. A more sophisticated architecture of the \texttt{NodeCollection} could than be designed to support modularity and make querying the model and updating their state and parameter blocks possible even if the model is not fully built yet.

\subsection*{Vectorization: Updating containers instead of single nodes}

The \texttt{update()} function and other important functionalities related to the simulation do not efficiently utilize the vectorized models yet. Each of these functions handles the instances of the different models separately and does not use the containers that represent all instances of each model. The NestKernel must be re-designed to use the containers as units for the whole simulation. Instead of storing a list of instances of the \texttt{Node} class, we now store instances of \texttt{VectorizedNode}s and each container executes then the same instructions in parallel for all the nodes it represents.

\subsection*{The JIT Orchestrator}

After removing the \texttt{NodeCollectionProxy} with the introduced modularity, the JIT module will only have the task of assembling the pieces of the model. It makes sure that the neuron model is connected with the correct synapse type without the necessity to run the code generation pipeline again to match the synapse type with the correct neuron model.


Having custom written models built and loaded automatically at the runtime with any pre-configurations in the simulation script,  neuroscientists can enjoy all benefits of the code generation pipeline without any manual interaction, which results in more time for research and less time for typing.
\cleardoublepage
