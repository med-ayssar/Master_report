%\vspace*{2cm}
% Abstract
%{\bf\Large Kurzfassung} \\ [1em] 
%Eine kurze Zusammenfassung der Arbeit.

\begin{abstract}

\emph{NEST} \citep{gewaltig2007nest, spreizer_sebastian_2022_6368024} is a simulator for large-scale networks of spiking and non-spiking neural networks used in computational neuroscience to build models of brain-like or brain-inspired neuronal circuits. It profits from the efficiency of C++ to simulate networks with large numbers of neurons and synaptic connections and is parallelized using MPI and OpenMP. By providing a Python interface \citep[\emph{PyNEST};][]{10.3389/neuro.11.012.2008} that interacts with the \emph{NestKernel} in C++ \citep{10.3389/fninf.2014.00023}, users can easily define their simulations and add new models dynamically at runtime by means of the function \texttt{nest.Install()}.

Originally, models had to be manually written in C++. Since some years, the model creation process is supported by \emph{NESTML} \citep{plotnikov2016nestml}, which generates the required code from a high-level specification and compiles them into dynamic libraries that are ready to be used from within \emph{PyNEST}. NESTML is a user-friendly, flexible and Turing-complete domain-specific language. It simplifies the modeling process for neuroscientists by allowing them to express their models in domain terminology, both with and without prior training in computer science. By specifying the target platform -- either a different hardware or new supported simulator -- the user can use the same model to generate code for the given target without any manual interventions. 

Running a simulation script in NEST using an external custom model requires the user to provide certain configurations to the NESTML code generator. Such configurations cover the model source code location, model dependencies to other models (e.g., custom synapse models) and the target platform. Although the NESTML code generation is completely automated, the user still has to explicitly invoke the NESTML interface for the model to be processed and compiled into an extension module for NEST, which is especially cumbersome if many neuron/synapse combinations are used.

The first goal of this thesis is to eliminate the explicit calls to the NESTML interface in the simulation script by a seamless extension to PyNEST that intercepts calls to \texttt{nest.Create()}  and \texttt{nest.Connect()} and controls the workflow logic of the NESTML functions behind the scenes. Depending on certain conditions, this integration decides if the model should be instantiated right away, or if the code generation and compilation can be delayed to a later point in time. As the workflow is only triggered when the user invokes specific PyNEST functions instead of invoking NESTML upfront, this extension is called just-in-time compilation (\emph{JIT}).

One problem that arises from a delayed availability of the model instances when using JIT is that model parameters cannot be queried before the actual start of the simulation. This prohibits adaptive parameter choices for following calls to PyNEST and thus restricts the flexibility for the users. A possible solution would be to cache parameters on the Python level until the instances are available, albeit at the cost of doubling the memory requirements. A more efficient solution is to provide a model-independent storage of parameters in the form of data vectors. This second solutions also vastly increases the cache-efficiency of models by applying a single-instruction-multiple-data \citep[\emph{SIMD};][]{nuzman2006auto} paradigm that increases the performance of the simulation script by storing all objects as a structure of arrays (\emph{SoA})

The second part of this thesis thus focuses on extending the NestKernel to support \emph{vectorization} by modifying the models to become the central unit for controlling the state of node instances: instead of storing attributes directly in the node instances, the vectorized model contains attributes in the form of \emph{vectors}, and the size of these vectors corresponds to the number of instances created during the simulation of that particular model type.

Together, \emph{JIT} and \emph{vectorization} result in a much more modular NestKernel in which \emph{JIT} is responsible for generating and assembling the components of the model into executable code, while the \emph{vectorization} is responsible for linking the components together in a way that can be efficiently run on all machines from laptops to supercomputers.

\end{abstract}

\cleardoublepage
